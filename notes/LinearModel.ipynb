{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to measure the performace of a model\n",
    "Our learned function need to generalized to new data(Generalization ability).\n",
    "How to measure this,We just need to calculate the error(loss) on test dataset.\n",
    "The shape of test error like a parabola. \n",
    "This is because: even the model's tranning error is lower and lower, the model is overfitted.\n",
    "It means the complexity of model is much higher than the true distribution of data.\n",
    "\n",
    "### How to avoid overfitting and build a good model\n",
    "To avoid this happening, we can add a regularization items on model which indicated the complexity of model.\n",
    "\n",
    "How to choose the best model:\n",
    "Cross-Validation\n",
    "K-fold CV\n",
    "\n",
    "* Feature selection\n",
    "Another way to get a sparse predictor: pick out a small set of the most\n",
    "relevant features\n",
    "A set of features F ⊆ {X1, . . . , XD} is minimally relevant\n",
    "if there is some h definable using F such that\n",
    "no h' defined on a superset of F has lower generalization loss\n",
    "no h\" defined on a subset of F has higher generalization loss\n",
    "Any feature not in a minimally relevant set is irrelevant\n",
    "Problems in choosing a minimally relevant set:\n",
    "– inaccurate estimate of generalization loss\n",
    "⇒ some features appear relevant when they’re not\n",
    "– NP-hard to find a set even with perfect estimates\n",
    "Forward selection: greedily add feature that decreases CV error most\n",
    "Backward selection: greedily delete feature that decreases CV error most\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization method \n",
    "* L1 regularization \n",
    "* L2 regularization\n",
    "* Ridge regression(L2)\n",
    "* Lasso regression(L1)\n",
    "* MAP interpretation\n",
    "MAP is identical to L2 regularization or\n",
    "LASSO = MAP learning with Laplacian prior\n",
    "[links](https://people.eecs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2004.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what's the number of parameters of multi-logistic model\n",
    "input is 28*28 image, and the output is ten classes(use softmax as classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The limitation of linear model and its advantages\n",
    "Can't resovle the XOR problem\n",
    "The computation process is very efficient(GPU)\n",
    "Linear operations is very stable\n",
    "    It means a small change in the input can never yield a big change in output\n",
    "Also,the derivative is very nice too.The result of derivative of linear model is always constants\n",
    "So, What we need to do is make a more sophisticated model based on linear model,and keep its goog properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activate functions\n",
    "* ReLUs(Rectifield Linear Units)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back propagation Algorithms\n",
    "1.What's the Chain rule\n",
    "2.How the data pipeline help reduce the workload \n",
    "3.Back propagation\n",
    "4.Back propagation will need twice more space for computing and storage,Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Deep network is working\n",
    "Perceptron can be seen as a linear model,but it can't resovle the XOR problem.\n",
    "However if we add the hidden layer of network to make it a deep one,It will solve the XOR problem\n",
    "Actually, enough linear models can fit any type of functions,such as circles and even more sophisticated shapes\n",
    "Intuitively,the deep network can solve any type of classifier problem,and it will be very qualified for image problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "Skinny jean anology problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout \n",
    "Make some activation input as 0 randomly.\n",
    "It seems a bad method but in fact it works very well to preventing the overfitting\n",
    "If this method doesn't work well, you may need a much bigger network\n",
    "One trick of Dropout is make the other part of input double. So the average of inputs is close to the original input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistic invariance\n",
    "The invariance in image including transform,scala,rotate invariance and so on.\n",
    "It means these input can share weights with each other if they obey statistic invariance and has no influence on output\n",
    "This idea is the key of build CNN and RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNets and Image processing\n",
    "Here the process of doing convolution is very similiar to the process in the computer vision\n",
    "We take a patch(kernel or sliding window) of the image and move this patch along the image. All patches share same weights and after processing ,we get a convolution layer.\n",
    "The size of convolution layer is up to many factors, the size of kernel,the step very patch move, and the way we deal with the board(valid padding/same padding or other padding technique)\n",
    "The structure of patch to k output is a tiny network.\n",
    "There are some importance concepts: Feature maps;input depth;patch/kernel;k(output depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced method for ConvNets\n",
    "1.Pooling\n",
    "  This is inherited from computer vision\n",
    "2.1\\*1 convolution\n",
    "3.Inception"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
