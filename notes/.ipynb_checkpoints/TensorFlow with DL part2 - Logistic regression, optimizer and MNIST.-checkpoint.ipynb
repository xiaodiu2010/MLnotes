{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model example and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model\n",
    "\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import xlrd\n",
    "DATA_FILE = \"data/fire_theft.xls\"\n",
    "# Step 1: read in data from the .xls file\n",
    "book = xlrd.open_workbook(DATA_FILE, encoding_override=\"utf-8\")\n",
    "sheet = book.sheet_by_index(0)\n",
    "data = np.asarray([sheet.row_values(i) for i in range(1, sheet.nrows)])\n",
    "n_samples = sheet.nrows - 1\n",
    "# Step 2: create placeholders for input X (number of fire) and label Y (number of\n",
    "theft)\n",
    "X = tf.placeholder(tf.float32, name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, name=\"Y\")\n",
    "# Step 3: create weight and bias, initialized to 0\n",
    "w = tf.Variable(0.0, name=\"weights\")\n",
    "b = tf.Variable(0.0, name=\"bias\")\n",
    "# Step 4: construct model to predict Y (number of theft) from the number of fire\n",
    "Y_predicted = X * w + b\n",
    "# Step 5: use the square error as the loss function\n",
    "loss = tf.square(Y - Y_predicted, name=\"loss\")\n",
    "# Step 6: using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    "with tf.Session() as sess:\n",
    "    # Step 7: initialize the necessary variables, in this case, w and b\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Step 8: train the model\n",
    "    for i in range(100): # run 100 epochs\n",
    "        for x, y in data:\n",
    "            # Session runs train_op to minimize loss\n",
    "            sess.run(optimizer, feed_dict={X: x, Y:y})\n",
    "    # Step 9: output the values of w and b\n",
    "    w_value, b_value = sess.run([w, b])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see the design pattern of Tensorflow which aims to separate data with graph and make graph reuseful.\n",
    "If we want to build a another model, we just need to rewrite like this: \n",
    "``` python\n",
    "# Step 3: create variables: weights_1, weights_2, bias. All are initialized to 0\n",
    "w = tf.Variable(0.0, name=\"weights_1\")\n",
    "u = tf.Variable(0.0, name=\"weights_2\")\n",
    "b = tf.Variable(0.0, name=\"bias\")\n",
    "# Step 4: predict Y (number of theft) from the number of fire\n",
    "Y_predicted = X * X * w + X * u + b\n",
    "# Step 5: Profit!\n",
    "```\n",
    "Here, optimizer is a tensorflow op which can be run in sess.run()\n",
    "Its goal is to minimize the loss(L2),and loss depends on variables w and b. From the graph, you can see that the giant node GrandientDescentOptimizer depends on 3 nodes: weights, bias, and gradients (which is automatically taken care of for us,我觉得这很容把模型建残啊！！). \n",
    "![Paste_Image.png](http://upload-images.jianshu.io/upload_images/733189-e798f4b0ea43168a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "WTF! 我只是建一个LINEAR MODEL就写成这样，🔑写更复杂的NN岂不是💊！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientDescentOptimizer means that our update rule is gradient descent. TensorFlow does auto differentiation for us, then update the values of w and b to minimize the loss. Autodiff is amazing!\n",
    "By default, the optimizer trains all the trainable variables whose objective function depend on. If there are variables that you do not want to train, you can set the keyword trainable to False when you declare a variable. One example of a variable you don’t want to train is the variable global_step, a common variable you will see in many TensorFlow model to keep track of how many times you’ve run your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer classes automatically compute derivatives on your graph, but creators of new Optimizers or expert users can call the lower-level functions below.\n",
    "``` python\n",
    "tf.gradients(ys, xs, grad_ys=None, name='gradients',\n",
    "colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None)\n",
    "``` \n",
    "Technical detail: This is especially useful when training only parts of a model. For example, we can use tf.gradients() for to take the derivative G of the loss w.r.t. to the middle layer. Then we\n",
    "use an optimizer to minimize the difference between the middle layer output M and M + G. This only updates the lower half of the network.\n",
    "(然而我还看不出它有什么luan用)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The list of optimizer of tensorflow](https://www.tensorflow.org/api_docs/python/train/):\n",
    "``` python\n",
    "tf.train.GradientDescentOptimizer  \n",
    "tf.train.AdadeltaOptimizer  \n",
    "tf.train.AdagradOptimizer  \n",
    "tf.train.AdagradDAOptimizer\n",
    "tf.train.MomentumOptimizer\n",
    "tf.train.AdamOptimizer\n",
    "tf.train.FtrlOptimizer\n",
    "tf.train.ProximalGradientDescentOptimizer\n",
    "tf.train.ProximalAdagradOptimizer\n",
    "tf.train.RMSPropOptimizer\n",
    "```\n",
    "About these optimizers, this [blog](http://sebastianruder.com/optimizing-gradient-descent/) is a very good reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "几乎所有的Deep Learning的介绍都是从MNIST的Logistic Regression开始的~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ../data/mnist/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ../data/mnist/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ../data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ../data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "MNIST = input_data.read_data_sets(\"../data/mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train this model,considering the number of data,we need batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "0.527759\n",
      "0.563789\n",
      "0.615246\n",
      "0.633535\n",
      "0.457059\n",
      "0.593228\n",
      "0.537054\n",
      "0.33487\n",
      "0.36831\n",
      "0.439739\n",
      "0.322416\n",
      "0.395333\n",
      "0.554289\n",
      "0.338763\n",
      "0.402002\n",
      "0.453394\n",
      "0.338088\n",
      "0.32536\n",
      "0.381439\n",
      "0.476234\n",
      "0.352113\n",
      "0.324997\n",
      "0.344775\n",
      "0.266603\n",
      "0.394915\n",
      "Accuracy 0.9099\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder data/mnist\n",
    "MNIST = input_data.read_data_sets(\"../data/mnist\", one_hot=True)\n",
    "\n",
    "# Step 2: Define parameters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "n_epochs = 25\n",
    "# Step 3: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9.\n",
    "# each label is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784])\n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10])\n",
    "\n",
    "# Step 4: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name=\"weights\")\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 5: predict Y from X and w, b\n",
    "# the model that returns probability distribution of possible label of the image\n",
    "# through the softmax layer\n",
    "# a batch_size x 10 tensor that represents the possibility of the digits\n",
    "logits = tf.matmul(X, w) + b\n",
    "# Step 6: define loss function\n",
    "# use softmax cross entropy with logits as the loss function\n",
    "# compute mean cross entropy, softmax is applied internally\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits, Y)\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over examples in the batch\n",
    "\n",
    "# Step 7: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize cost\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    n_batches = int(MNIST.train.num_examples/batch_size)\n",
    "    for i in range(n_epochs): # train the model n_epochs times\n",
    "        for _ in range(n_batches):\n",
    "            X_batch, Y_batch = MNIST.train.next_batch(batch_size)\n",
    "            _, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch})\n",
    "        print loss_batch\n",
    "    n_batches = int(MNIST.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = MNIST.test.next_batch(batch_size)\n",
    "        _, loss_batch, logits_batch = sess.run([optimizer, loss, logits],\n",
    "        feed_dict={X: X_batch, Y:Y_batch})\n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32)) \n",
    "        total_correct_preds += sess.run(accuracy)\n",
    "    print \"Accuracy {0}\".format(total_correct_preds/MNIST.test.num_examples)\n",
    "# average loss should be around 0.35 after 25 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果这个loss不收敛啊你敢信！！然而它的accuracy高达91%！\n",
    "TensorFlow，一个神奇的Deep Learning工具！ 你值得拥有！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf_2.7",
   "language": "python",
   "name": "tensorflow2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
